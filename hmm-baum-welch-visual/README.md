# Hidden Markov Model (HMM) using Baumâ€“Welch Algorithm â€” IMPLEMENTED USING JAVASCRIPT

**Name:** SREE LEKSHMI H
**Registration Number:** TCR24CS067

---

## ðŸ“Œ Project Overview

This project implements a **Hidden Markov Model (HMM)** trained using the **Baumâ€“Welch algorithm (Expectationâ€“Maximization)** entirely in **JavaScript** with a fully interactive **HTML/CSS frontend**.

The system allows users to:

* Enter observation sequences
* Train an HMM model
* Visualize parameters
* See convergence graphs
* Display state transition diagrams
* Decode hidden states using Viterbi algorithm

No backend or external libraries are required â€” everything runs in the browser.

---

## ðŸŽ¯ Objectives

* Implement HMM mathematically from scratch
* Apply Baumâ€“Welch parameter estimation
* Visualize learning behavior
* Demonstrate probabilistic sequence modeling

---

##  Concepts Implemented

The project implements the three classical HMM problems:

### 1ï¸âƒ£ Evaluation Problem

Compute likelihood
[
P(O \mid \lambda)
]
using **Forward Algorithm**

---

### 2ï¸âƒ£ Decoding Problem

Find most likely hidden state sequence using:

**Viterbi Algorithm**

---

### 3ï¸âƒ£ Learning Problem

Estimate model parameters using:

**Baumâ€“Welch Algorithm (EM)**

---

## âš™ï¸ Algorithms Used

* Forward algorithm (scaled version)
* Backward algorithm
* Gamma computation
* Xi computation
* Baumâ€“Welch parameter update
* Viterbi decoding
* Random initialization with seed

---

## ðŸ“Š Features

âœ” Interactive UI
âœ” Adjustable parameters (states, iterations, tolerance)
âœ” Random seed for reproducibility
âœ” Log-likelihood convergence graph
âœ” Transition matrix visualization
âœ” Emission matrix visualization
âœ” HMM state diagram with probabilities
âœ” Works fully offline

---

## ðŸ–¥ï¸ Technologies Used

* HTML5
* CSS3
* Vanilla JavaScript (ES6)
* SVG for diagram rendering
* Canvas API for plotting

---

## ðŸš€ How to Run

1. Download or clone repository
2. Open folder
3. Double-click

```
index.html
```

OR

Use VS Code Live Server extension.

No installation required.

---

## ðŸ“ˆ Example Input

```
W H H W H
```

Output shows:

* learned Ï€
* transition matrix A
* emission matrix B
* convergence graph
* predicted hidden states

---

## ðŸ§¾ Mathematical Model

An HMM is defined by:

[
\lambda = (A, B, \pi)
]

Where:

* **A** = transition probability matrix
* **B** = emission probability matrix
* **Ï€** = initial state distribution

---

## ðŸ” Baumâ€“Welch Learning Principle

Observations â†’ Hidden State Beliefs â†’ Expected Counts â†’ Parameter Updates

The algorithm iteratively maximizes:

[
\max_\lambda P(O \mid \lambda)
]

until convergence.

---


## Applications of HMM

* Speech recognition
* Part-of-speech tagging
* Activity recognition
* Bioinformatics
* Financial modeling

---

## Key Insight

> Observations do not directly change probabilities.
> They reshape belief distributions, which update model parameters.

---

## License

Educational use only.

---

## Author
SREE LEKSHMI H
